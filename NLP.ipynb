{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCDwiyGYPlIiL9iV7pRSNh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trinity-SYT-SECURITY/NLP_jieba/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!git clone https://github.com/fxsjy/jieba.git\n",
        "!pip install whoosh"
      ],
      "metadata": {
        "id": "-3dCKx6SlgtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz7Ww8xilX7u"
      },
      "outputs": [],
      "source": [
        "from io import StringIO\n",
        "import io\n",
        "import os.path\n",
        "import csv\n",
        "import jieba\n",
        "import jieba.analyse\n",
        "import string\n",
        "import logging\n",
        "import jieba.posseg as pseg\n",
        "#from jieba.analyse import ChineseAnalyzer\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "srcPath = \"/content/testzh\"#請確保該目錄下有要進行關鍵字提取的文本，自行更改正確路徑\n",
        "files = [f for f in os.listdir(srcPath) if f.endswith(\".txt\")]\n",
        "data=[]\n",
        "jieba.initialize()\n",
        "\n",
        "#預處理\n",
        "def preprocess_text(text):\n",
        "    # 去除.跟數字'[^*&%#@^$\"\\]'\n",
        "    pattern = re.compile(r'[{}]')\n",
        "    #pattern = re.split(\"(?<!\\d)\\.(?!.\\d)\")\n",
        "    text = re.sub(pattern, '', text)\n",
        "    stopword_set = set()#載入斷詞\n",
        "    tokens = [word for word in text.split() if word not in stopword_set]\n",
        "    \n",
        "    return \" \".join(tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install -c conda-forge openjdk python=3.8 jpype1=0.7.0 -y\n",
        "!pip install pyhanlp\n",
        "\n",
        "from pyhanlp import *\n",
        "\n"
      ],
      "metadata": {
        "id": "yneX2xFW_HhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import jieba\n",
        "\n",
        "file_results = {}  # Dictionary to store the search results for each file\n",
        "\n",
        "for file in files:\n",
        "    with open(os.path.join(srcPath, file), \"r\", encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        text = preprocess_text(text)\n",
        "\n",
        "        #text = re.sub(r\"(\\d+)\\.(\\d+)\\.(\\d+)\", r\"\\1\\2\\3\", text)  \n",
        "        tokens = jieba.tokenize(text)\n",
        "        result = []\n",
        "        for tk in tokens:\n",
        "            word = tk[0]\n",
        "            start = tk[1]\n",
        "            end = tk[2]\n",
        "            if '.' in word and all(c.isdigit() or c == '.' for c in word):\n",
        "                # preserve dots in numbers\n",
        "                for subword in word.split('.'):\n",
        "                    result.append((subword, start, start+len(subword)))\n",
        "                    start += len(subword)\n",
        "            else:\n",
        "                result.append((word, start, end))\n",
        "        #text = jieba.lcut_for_search(str(text),HMM=True)\n",
        "        # join words and print result\n",
        "        result = [r[0] for r in result]\n",
        "        \n",
        "        text = ''.join(text)\n",
        "        print(text)\n",
        "        file_results[file] = text  # Store the tokenized text for this file\n",
        "\n",
        "while True:\n",
        "    user = input(\"Input search Producename: \")\n",
        "    user_results = {}  # Dictionary to store the search results for each file\n",
        "\n",
        "    for file, text in file_results.items():\n",
        "        if user in text:\n",
        "            user_results[file] = text.count(user)\n",
        "\n",
        "    if len(user_results) == 0:\n",
        "        print(\"No files found containing search term.\")\n",
        "    else:\n",
        "        for file, count in user_results.items():\n",
        "            print(\"Found\", user, \"in file\", file, \":\", count, \"times\")\n",
        "\n",
        "    answer = input(\"Do you want to search for another product name? (y/n)\")\n",
        "    if answer.lower() != \"y\":\n",
        "        break\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "lqLJjyYTydKD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}